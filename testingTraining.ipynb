{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from data_logger import DataLogger\n",
    "from dqn_agent import Agent\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading parameters\n",
    "ep, lp, hp = get_params() #params[\"environment\"], params[\"logging\"], params[\"hyperparameters\"]\n",
    "hp_algo = hp['dqn/ddqn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_env(game=ep[\"game\"], level=ep[\"level\"], action_space=ep[\"action_space\"])\n",
    "env = apply_wrappers(env, skip=ep[\"skip\"], gray_scale=ep[\"gray_scale\"], shape=ep[\"frame_shape\"], num_stack=ep[\"num_stack\"], buffer=ep[\"buffer\"])\n",
    "next_state = env.reset()\n",
    "print(next_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import *\n",
    "next_state, reward, done, info = env.step(get_action_sample(env))\n",
    "plot_sequence_observations(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env, batch_size=hp_algo[\"batch_size\"], buffer_size=hp_algo[\"buffer_size\"], learning_rate=hp_algo[\"learning_rate\"], observation_size=(ep[\"num_stack\"],)+tuple(ep[\"frame_shape\"]), action_size=get_action_space_size(env), \n",
    "              discount_factor=hp_algo[\"discount_factor\"], model=hp_algo[\"model\"], epsilon_min=hp_algo[\"exploration_min\"], epsilon_decay=hp_algo[\"exploration_decay\"], update_online_every=hp_algo[\"train_freq\"],\n",
    "              update_target_from_online_every=hp_algo[\"target_update_interval\"], start_learning_after=hp_algo[\"learning_starts\"])\n",
    "logger = DataLogger(env, hp, agent, model=hp_algo[\"model\"])\n",
    "\n",
    "for episode in tqdm(range(hp_algo[\"num_episodes\"])):\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action_idx = agent.act(state)\n",
    "\n",
    "        next_state, reward, done, info = env.step(get_action(action_idx, env))\n",
    "\n",
    "        agent.append_experience_to_memory(state, next_state, action_idx, reward, done)\n",
    "        \n",
    "        loss = agent.learn()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "        logger.time_step(reward, loss, n_time_steps_save_model=lp[\"n_time_steps_save_model\"])\n",
    "\n",
    "    logger.episode_step(info)\n",
    "\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing trained model\n",
    "env = get_env(game=ep[\"game\"], level=ep[\"level\"], action_space=ep[\"action_space\"])\n",
    "env = apply_wrappers(env, skip=ep[\"skip\"], gray_scale=ep[\"gray_scale\"], shape=ep[\"frame_shape\"], num_stack=ep[\"num_stack\"], buffer=ep[\"buffer\"])\n",
    "\n",
    "agent = Agent(env, batch_size=hp_algo[\"batch_size\"], buffer_size=hp_algo[\"buffer_size\"], learning_rate=hp_algo[\"learning_rate\"], observation_size=(ep[\"num_stack\"],)+tuple(ep[\"frame_shape\"]), action_size=get_action_space_size(env), \n",
    "              discount_factor=hp_algo[\"discount_factor\"], model=hp_algo[\"model\"], epsilon_min=hp_algo[\"exploration_min\"], epsilon_decay=hp_algo[\"exploration_decay\"], update_online_every=hp_algo[\"train_freq\"],\n",
    "              update_target_from_online_every=hp_algo[\"target_update_interval\"], start_learning_after=hp_algo[\"learning_starts\"])\n",
    "agent.load(r\"logs/SuperMarioBros-1-1-v0/DDQN/20240123234603/checkpoints/best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    while True:\n",
    "        time_stamp=0\n",
    "        state = env.reset()\n",
    "        while time_stamp<1e4:\n",
    "            action_idx = agent.act(state)\n",
    "\n",
    "            next_state, reward, done, info = env.step(get_action(action_idx, env))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            env.render()\n",
    "            \n",
    "            time_stamp += 1\n",
    "\n",
    "            if done or info[\"flag_get\"]:\n",
    "                break\n",
    "except:\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
